{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maryam-Afshari/thesis/blob/master/xgboost_Lobith.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEcZL5LXl3m8",
        "outputId": "64e07ccf-156a-4d59-896a-948c1f36e6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 19 07:20:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# checking out the GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMz95ceql4qp",
        "outputId": "da7464ac-39e3-4746-efb2-722506ce1a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "# checking out the memory\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7kyCtFRstXqI"
      },
      "outputs": [],
      "source": [
        "#!pip install hydroeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhWM3ObX1I0Y"
      },
      "source": [
        "# Running XGBoost model on Lobith station"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4KeOfoe_oc-3"
      },
      "outputs": [],
      "source": [
        "# required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hydroeval as he\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost.sklearn import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zID4dw-3t2lm",
        "outputId": "898b30e3-1591-4b3f-f641-af6849758237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_4g5OuacuHil"
      },
      "outputs": [],
      "source": [
        "# reading data \n",
        "lobith = pd.read_csv('/content/drive/MyDrive/Thesis-Afshari/data/lobith.csv',index_col=0)\n",
        "q_lobith = pd.read_csv(\"/content/drive/MyDrive/Thesis-Afshari/data/q_lobith.csv\",index_col=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqY_pYwBu5YM",
        "outputId": "418d97b6-3a93-49a4-cb6c-53d3e900ad1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  if self.run_code(code, result):\n"
          ]
        }
      ],
      "source": [
        "# making lagged variables\n",
        "\n",
        "# define the number of lags\n",
        "TIME_STEPS = 55\n",
        "\n",
        "#add the lagged variables for predictors to the dataframe\n",
        "for i, var in enumerate(lobith[[\"et\",\"p\",\"t\"]]):\n",
        "  for step in range(0, TIME_STEPS - 1):\n",
        "    lobith.insert(i*(TIME_STEPS) + 1,\n",
        "                 f'{var}_lag_{TIME_STEPS - 1 - step}',\n",
        "                 lobith[var].shift(TIME_STEPS - 1 - step))\n",
        "\n",
        "#remove the first TIME_STEPS - 1 rows since they will contain NA values\n",
        "lobith = lobith.iloc[TIME_STEPS - 1:,:].reset_index(drop=True)\n",
        "q_lobith = q_lobith.iloc[TIME_STEPS - 1:,:].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7oZ0J7fvFbQ"
      },
      "source": [
        "# Train_test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yNTnfzk5vA3d"
      },
      "outputs": [],
      "source": [
        "# train-test split\n",
        "len_ = int(0.75 * lobith.shape[0])\n",
        "\n",
        "df_train = lobith[:len_]\n",
        "df_test = lobith[len_:]\n",
        "\n",
        "# x train without lagged vars\n",
        "X_train = df_train[[\"et\",\"t\",\"p\"]]\n",
        "\n",
        "# x train with lagged vars\n",
        "X_train_lagged = df_train.drop(columns = [\"obs\",\"datetime\"], axis =1)\n",
        "y_train = df_train.obs\n",
        "\n",
        "# x test without lagged vars\n",
        "X_test = df_test[[\"et\",\"t\",\"p\"]]\n",
        "\n",
        "# x test with lagged vars\n",
        "X_test_lagged = df_test.drop(columns = [\"obs\",\"datetime\"], axis =1)\n",
        "y_test = df_test.obs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVC7VRJbvUso"
      },
      "source": [
        "### Hyperparameter grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27LzAv4ZvSkQ",
        "outputId": "66ece3de-135f-428e-ecf2-293a67e4f698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "best score for XGBoost model without lagged variables is : -0.11648620281568008\n",
            "best parameters XGBoost model without lagged variables are: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 4, 'n_estimators': 800, 'nthread': 1, 'objective': 'reg:squarederror', 'subsample': 0.8}\n",
            "CPU times: user 4.63 s, sys: 373 ms, total: 5 s\n",
            "Wall time: 4min 35s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Various hyper-parameters to tune for XGBoost model without input lagged variables \n",
        "\n",
        "xgb = XGBRegressor()\n",
        "parameters = {'nthread':[1], \n",
        "              'objective':['reg:squarederror'],\n",
        "              'learning_rate': [.01, .05, .1], \n",
        "              'max_depth': [7, 9], \n",
        "              'min_child_weight': [4],\n",
        "              'subsample': [0.8, 0.9], \n",
        "              'colsample_bytree': [0.7, 0.9], \n",
        "              'n_estimators': [800, 1000]} \n",
        "\n",
        "xgb_grid = GridSearchCV(xgb,\n",
        "                        parameters,\n",
        "                        cv = 5,\n",
        "                        n_jobs = 5,\n",
        "                        verbose=True)\n",
        "\n",
        "\n",
        "xgb_grid.fit(X_train,\n",
        "         y_train)\n",
        "\n",
        "print(f\"best score for XGBoost model without lagged variables is : {xgb_grid.best_score_}\")\n",
        "print(f\"best parameters XGBoost model without lagged variables are: {xgb_grid.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ij3GHRWwn5o",
        "outputId": "59118117-5706-4ed5-96e0-ca4e5e0e7e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "best score for XGBoost model with 55 lagged variables is : -0.11648620281568008\n",
            "best parameters XGBoost model with 55 lagged variables are: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 4, 'n_estimators': 800, 'nthread': 1, 'objective': 'reg:squarederror', 'subsample': 0.8}\n",
            "CPU times: user 2min 6s, sys: 4.27 s, total: 2min 10s\n",
            "Wall time: 2h 3min 2s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Various hyper-parameters to tune foe XGBoost model with 55 input lagged variables \n",
        "\n",
        "xgb = XGBRegressor()\n",
        "parameters = {'nthread':[1], \n",
        "              'objective':['reg:squarederror'], \n",
        "              'learning_rate': [.01, .05, .1], \n",
        "              'max_depth': [7, 9], \n",
        "              'min_child_weight': [4],\n",
        "              'subsample': [0.8, 0.9], \n",
        "              'colsample_bytree': [0.7, 0.9], \n",
        "              'n_estimators': [800, 1000]} \n",
        "\n",
        "xgb_grid_lagged = GridSearchCV(xgb,\n",
        "                        parameters,\n",
        "                        cv = 5,\n",
        "                        n_jobs = 5,\n",
        "                        verbose=True)\n",
        "\n",
        "\n",
        "xgb_grid_lagged.fit(X_train_lagged,\n",
        "         y_train)\n",
        "\n",
        "print(f\"best score for XGBoost model with 55 lagged variables is : {xgb_grid.best_score_}\")\n",
        "print(f\"best parameters XGBoost model with 55 lagged variables are: {xgb_grid.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uszGWHXVb7yF"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zN4qNs_pb6KQ"
      },
      "outputs": [],
      "source": [
        "# define the model for without lagged variables model\n",
        "model =  xgb_grid.best_estimator_\n",
        "\n",
        "# fit model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make a prediction\n",
        "yhat = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kovsl0GPb6nP"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "model.save_model('/content/drive/MyDrive/Thesis-Afshari/output/xgboost_without_lag.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P4MSJIWZco2K"
      },
      "outputs": [],
      "source": [
        "# define the model for with 60 lagged variables model\n",
        "model_lagged =  xgb_grid_lagged.best_estimator_\n",
        "\n",
        "# fit model\n",
        "model_lagged.fit(X_train_lagged, y_train)\n",
        "# make a prediction\n",
        "yhat_lagged = model_lagged.predict(X_test_lagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wONiSe-Qc8k3"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "model_lagged.save_model('/content/drive/MyDrive/Thesis-Afshari/output/xgboost_with_55lag.model')\n",
        "\n",
        "# save prediction with lagged \n",
        "np.save(\"/content/drive/MyDrive/Thesis-Afshari/output/ypred_lobith_xgboost_55lag.npy\", yhat_lagged, allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfdLWIcGec4e"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZWwCxO8eYNM",
        "outputId": "d4f01fba-7168-491e-d155-75241644de8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nse and kge of the PCR model are 0.36 and 0.55.\n",
            "The nse and kge of the xgboost model are 0.029 and 0.094.\n",
            "The nse and kge of the xgboost model including lagged variables are 0.69 and 0.67.\n"
          ]
        }
      ],
      "source": [
        "# evaluate pcr model using nse and kge\n",
        "streamflow_pcr = q_lobith.pcr[len_:]\n",
        "nse_pcr = he.evaluator(he.nse, streamflow_pcr, y_test)\n",
        "kge_pcr, r, alpha, beta = he.evaluator(he.kge, streamflow_pcr, y_test)\n",
        "print(\"The nse and kge of the PCR model are {:.2} and {:.2}.\".format(nse_pcr[0], kge_pcr[0]))\n",
        "\n",
        "# evaluate the prediction using nse and kge for model with only meteorological variables\n",
        "nse = he.evaluator(he.nse, yhat, y_test)\n",
        "\n",
        "kge, r, alpha, beta = he.evaluator(he.kge, yhat, y_test)\n",
        "print(\"The nse and kge of the xgboost model are {:.2} and {:.2}.\".format(nse[0], kge[0]))\n",
        "\n",
        "# evaluate the prediction using nse and kge for model with meteorological variables + lagged variables\n",
        "nse_lag = he.evaluator(he.nse, yhat_lagged, y_test)\n",
        "kge_lag, r_lag, alpha_lag, beta_lag = he.evaluator(he.kge, yhat_lagged, y_test)\n",
        "\n",
        "print(\"The nse and kge of the xgboost model including lagged variables are {:.2} and {:.2}.\".format(nse_lag[0], kge_lag[0]))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "xgboost_Lobith.ipynb",
      "provenance": [],
      "mount_file_id": "1cW2b-HR2sOCLLXNA5sI3Yql7IW_16zd1",
      "authorship_tag": "ABX9TyO1/lvp8GdQBRJuJWbenc93",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}