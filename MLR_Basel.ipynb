{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLR_Basel.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOg6/Gay6tB0XiFr98FyB/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maryam-Afshari/thesis/blob/master/MLR_Basel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking out the GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RIChmcHDWrX",
        "outputId": "9f9cfcbb-97ba-4fb0-ce3f-63b22103df04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 18 19:27:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking out the memory\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_2vRXpdDVrg",
        "outputId": "4af77cdd-c7e4-4aee-e1c2-afa2a31788b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running MLR on Basel station "
      ],
      "metadata": {
        "id": "HLQnWYWODdRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install hydroeval"
      ],
      "metadata": {
        "id": "j9SGN0thDKyc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import hydroeval as he\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zC9CgAOvDLHA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46USf4nxDLNh",
        "outputId": "7053a9ff-1341-48a5-d5f7-6f6b9b6098cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading data\n",
        "basel= pd.read_csv('/content/drive/MyDrive/Thesis-Afshari/data/basel.csv',index_col=0)\n",
        "q_basel = pd.read_csv(\"/content/drive/MyDrive/Thesis-Afshari/data/q_basel.csv\",index_col=0)\n"
      ],
      "metadata": {
        "id": "-YrGibLyDLQ7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering"
      ],
      "metadata": {
        "id": "mGD68aGREAky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making lagged variables\n",
        "\n",
        "# define the number of lags\n",
        "TIME_STEPS = 60\n",
        "\n",
        "#add the lagged variables for predictors to the dataframe\n",
        "for i, var in enumerate(basel[[\"et\",\"p\",\"t\"]]):\n",
        "  for step in range(0, TIME_STEPS - 1):\n",
        "    basel.insert(i*(TIME_STEPS) + 1,\n",
        "                 f'{var}_lag_{TIME_STEPS - 1 - step}',\n",
        "                 basel[var].shift(TIME_STEPS - 1 - step))\n",
        "\n",
        "#remove the first TIME_STEPS - 1 rows since they will contain NA values\n",
        "basel = basel.iloc[TIME_STEPS - 1:,:].reset_index(drop=True)\n",
        "q_basel = q_basel.iloc[TIME_STEPS - 1:,:].reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ62CpmLDLYb",
        "outputId": "610364f8-82a1-4758-c1f1-3e72127edba1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  if self.run_code(code, result):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-test split and defineing variables\n"
      ],
      "metadata": {
        "id": "RWpJFzzREJrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "len_ = int(0.75 * basel.shape[0])\n",
        "\n",
        "df_train = basel[:len_]\n",
        "df_test = basel[len_:]\n",
        "\n",
        "# x-train without lagged vars\n",
        "X_train = df_train[[\"et\",\"t\",\"p\"]]\n",
        "\n",
        "# x-train with lagged vars\n",
        "X_train_lagged = df_train.drop(columns = [\"obs\",\"datetime\"], axis =1)\n",
        "y_train = df_train.obs\n",
        "\n",
        "# x-test without lagged vars\n",
        "X_test = df_test[[\"et\",\"t\",\"p\"]]\n",
        "\n",
        "# x-test with lagged vars\n",
        "X_test_lagged = df_test.drop(columns = [\"obs\",\"datetime\"], axis =1)\n",
        "y_test = df_test.obs"
      ],
      "metadata": {
        "id": "KnWmAbs5EIUs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalising data"
      ],
      "metadata": {
        "id": "mF-JlJzzESTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalising perdictors \n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# normalising perdictors for model with the lagged varibales\n",
        "scaler_lagged = MinMaxScaler()\n",
        "X_train_lagged = scaler_lagged.fit_transform(X_train_lagged)\n",
        "X_test_lagged = scaler_lagged.transform(X_test_lagged)"
      ],
      "metadata": {
        "id": "tCJ5MGViEIbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "YGcNO2CUEubZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an object of LinearRegression class\n",
        "lm = LinearRegression()\n",
        "\n",
        "# fitting the training data\n",
        "lm.fit(X_train,y_train)\n",
        "\n",
        "# evaluating the model\n",
        "lm.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWVZa4G_E0xG",
        "outputId": "634038ff-2d6d-4098-9c0c-a0f0094f7f7b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12075166081013522"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an object of LinearRegression class\n",
        "lm_lag = LinearRegression()\n",
        "\n",
        "# fitting the training data for model with the lagged variables\n",
        "lm_lag.fit(X_train_lagged,y_train)\n",
        "\n",
        "# evaluating the model\n",
        "lm_lag.score(X_test_lagged, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdqdD_7NE2IO",
        "outputId": "59deea17-fcf0-4054-c646-19de57fa2fde"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6796257969847803"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "whndiHOSNHfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate pcr model using nse and kge\n",
        "streamflow_pcr = q_basel.pcr[len_:]\n",
        "nse_pcr = he.evaluator(he.nse, streamflow_pcr,y_test)\n",
        "kge_pcr, r_pcr, alpha_pcr, beta_pcr = he.evaluator(he.kge, streamflow_pcr, y_test)\n",
        "print(\"The nse and kge of the PCR model are {:.2} and {:.2} respectively.\".format(nse_pcr[0], kge_pcr[0]))\n",
        "\n",
        "# predicting for model with only meteorological variables\n",
        "y_pred =  lm.predict(X_test)\n",
        "\n",
        "# predicting for model with meteorological variables + lagged variables\n",
        "y_pred_lagged = lm_lag.predict(X_test_lagged)\n",
        "\n",
        "# evaluate the prediction using nse and kge for model with only meteorological variables\n",
        "nse = he.evaluator(he.nse, y_pred, y_test)\n",
        "kge, r, alpha, beta = he.evaluator(he.kge, y_pred, y_test)\n",
        "\n",
        "# evaluate the prediction using nse and kge for model with meteorological variables + lagged variables\n",
        "nse_lag = he.evaluator(he.nse, y_pred_lagged, y_test)\n",
        "kge_lag, r_lag, alpha_lag, beta_lag = he.evaluator(he.kge, y_pred_lagged, y_test)\n",
        "print(\"The nse and kge of the linear model are {:.2} and {:.2} respectively.\".format(nse[0], kge[0]))\n",
        "print(\"The nse and kge of the linear model including lagged variables are {:.2} and {:.2} respectively.\".format(nse_lag[0], kge_lag[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyhL2aVQE8D6",
        "outputId": "6190c1e7-c7f7-4dae-847d-b7f3ceb114a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nse and kge of the PCR model are 0.22 and 0.64 respectively.\n",
            "The nse and kge of the linear model are 0.12 and 0.12 respectively.\n",
            "The nse and kge of the linear model including lagged variables are 0.68 and 0.72 respectively.\n"
          ]
        }
      ]
    }
  ]
}